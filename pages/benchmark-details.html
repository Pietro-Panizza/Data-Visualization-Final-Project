<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Benchmarks Analysis - AI Performance Details</title>
    
    <link rel="stylesheet" href="benchmark-details.css">    
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;700&display=swap" rel="stylesheet">
    <script src="https://d3js.org/d3.v7.min.js"></script>
</head>
<body>
    
    <div class="detail-container">
        <!-- Link to go back to the homepage -->
        <a href="../index.html#section-benchmarks" class="back-link">
            <span class="back-arrow">←</span> Back to Homepage
        </a>
        
        
        <header class="detail-header">
            <h1 class="detail-title">AI Benchmarks: Detailed Performance Analysis</h1>
            <p class="detail-subtitle">
                An in-depth examination of how leading AI models perform across standardized evaluation metrics. 
                This analysis covers analytical and theoretical reasoning, scientific reasoning, knowledge recall, 
                strategic game and search based reasoning, software engineering and code repair, reliability over time.
            </p>
        </header>
        
        
        
        <!-- First plot section (polarchart) -->
        <section class="chart-section" id="polar-chart-section">
            <h2 class="chart-title">Model performance </h2>
            <p class="chart-description">
                Select an AI model to view its performance across different benchmarks.
                The radar chart shows normalized scores (0-1) for each benchmark the model has been tested on
            </p>
            
            <div class="chart-container polar-chart-container" id="polar-chart-1">
                <div class="chart-controls">
                    <div class="control-group">
                        <label for="polarModelSelect">Select AI Model:</label>
                        <select id="polarModelSelect" class="model-select">
                        </select>
                    </div>
                </div>
                <div class="model-info" id="polarModelInfo">
                    <div class="model-info-template">
                        <div class="model-details">
                            <div class="detail-row">
                                <span class="detail-label">Organization:</span>
                                <span class="detail-value" id="model-organization">-</span>
                            </div>
                            <div class="detail-row">
                                <span class="detail-label">Version:</span>
                                <span class="detail-value" id="model-version">-</span>
                            </div>
                            <div class="detail-row">
                                <span class="detail-label">Release Date:</span>
                                <span class="detail-value" id="model-releaseDate">-</span>
                            </div>
                            <div class="detail-row">
                                <span class="detail-label">Country:</span>
                                <span class="detail-value" id="model-country">-</span>
                            </div>
                            <div class="detail-row">
                                <span class="detail-label">Benchmarks Completed:</span>
                                <span class="detail-value" id="model-benchmarkCount">0/10</span>
                            </div>
                            <div class="detail-row">
                                <span class="detail-label">Average Score:</span>
                                <span class="detail-value" id="model-avgNormalizedScore">N/A</span>
                            </div>
                        </div>
                    </div>
                </div>

                <div class="chart-wrapper">
                    <svg id="polarChartSvg" ></svg>
                </div>
                <div class="chart-notes">
                    Data source: Epoch AI, 'AI Benchmarking Hub'. Published online at epoch.ai. Retrieved from <a href="https://epoch.ai/benchmarks/use-this-data">Epoch AI website</a>
                </div>

            </div>
            <div class="chart-analysis">
                <p>
                    The polar chart above illustrates the performance of each analyzed model across a selected set of benchmarks.
                    By analyzing scores across major labs (OpenAI, Google DeepMind, Anthropic, and Alibaba), several critical trends emerge. Notably, Gemini 3 Pro (November 2025) and GPT-5.2 
                    (late 2025) have established a new “gold standard,” achieving average scores exceeding 0.65. These models appear to have effectively addressed conventional 
                    academic benchmarks, attaining near-perfect accuracy on AIME 2025 and surpassing 90% performance on GPQA Diamond, a PhD-level science benchmark.
                    Despite these advances, FrontierMath continues to function as the most stringent discriminator of model capability. Even the strongest systems such as Gemini 3 Pro, 
                    which leads with almost 40%, exhibit substantial difficulty with research-level problems. This persistent gap underscores that genuine mathematical discovery 
                    remains an unconquered emergent capability rather than a solved competency.
                    The radar plots further reveal distinct “model identities.” The Claude 4.5 series from Anthropic demonstrates a strong emphasis on SWE-Bench and software engineering tasks, 
                    positioning it as a preferred choice for agentic coding and complex system architecture. The GPT-5 series from OpenAI exhibits the most balanced, general-purpose performance 
                    profile, with a modest advantage in WebDev Arena tasks and long-context knowledge work. Google's Gemini 3 models show clear dominance in visual reasoning, as well as in 
                    advanced scientific knowledge retrieval. Meanwhile, leading Chinese models—such as Qwen3-235B and DeepSeek-R1—have reached parity with proprietary 2024-era systems, 
                    consistently achieving average score values in the 0.50-0.57 range and demonstrating particular strength in mathematical chain-of-thought reasoning.
                </p>
            </div>

        </section>

        
        
        <!-- Second plot section (barchart) -->
        <section class="chart-section" id="bar-chart-section">
            <h2 class="chart-title">Benchmark Comparison</h2>
            <p class="chart-description">
                Select a benchmark to compare performance of all AI models. 
                This bar chart shows raw scores for each model on the selected benchmark.
            </p>
            
            <div class="chart-container bar-chart-container" id="bar-chart-1">
                <div class="chart-controls bar-chart-controls">
                    <div class="control-group">
                        <label for="barBenchmarkSelect">Select Benchmark:</label>
                        <select id="barBenchmarkSelect" class="benchmark-select">
                            <option value="" disabled selected>Loading benchmarks...</option>
                        </select>
                    </div>
                    <div class="control-group">
                        <label for="barModelSort">Sort By:</label>
                        <select id="barModelSort" class="sort-select">
                            <option value="score-desc">Score (High to Low)</option>
                            <option value="score-asc">Score (Low to High)</option>
                        </select>
                    </div>
                </div>
                
                <div class="benchmark-stats" id="benchmarkStats">
                    <div class="stats-grid">
                        <div class="stat-card">
                            <span class="stat-label">Models Compared:</span>
                            <span class="stat-value" id="stat-models-count">0</span>
                        </div>
                        <div class="stat-card">
                            <span class="stat-label">Highest Score:</span>
                            <span class="stat-value" id="stat-highest-score">0</span>
                        </div>
                        <div class="stat-card">
                            <span class="stat-label">Average Score:</span>
                            <span class="stat-value" id="stat-avg-score">0</span>
                        </div>
                        <div class="stat-card">
                            <span class="stat-label">Selected Benchmark:</span>
                            <span class="stat-value" id="stat-benchmark-name">-</span>
                        </div>
                    </div>
                </div>
                
                <div class="chart-wrapper bar-chart-wrapper">
                    <svg id="barChartSvg"></svg>
                </div>

            </div>
            
            <div class="chart-notes">
                Data source: Epoch AI, ‘AI Benchmarking Hub’. Published online at epoch.ai. Retrieved from <a href="https://epoch.ai/benchmarks/use-this-data">Epoch AI</a>
            </div>
            <div class="chart-analysis">
                <p>
                    The 2025-2026 benchmarking cycle signifies a clear inflection point from a prior “scaling era,” primarily defined by gains in general linguistic fluency, to a distinct 
                    “reasoning era” characterized by multi-step cognitive processing and increasing domain-specific autonomy. At the leading edge of performance, the GPT-5 and Gemini 3 Pro model
                    families have effectively established a new upper bound, saturating undergraduate-level science and competitive mathematics benchmarks—such as GPQA Diamond and AIME 2025—with 
                    performance levels exceeding 90%.Nevertheless, a substantial “reasoning gap” remains evident on FrontierMath. Even the most advanced architectures rarely exceed the 40% threshold 
                    on problems requiring research-grade mathematical reasoning, indicating a persistent boundary between expert-level knowledge retrieval and genuinely novel logical inference. 
                    This limitation suggests that current systems, while highly proficient in formalized domains, have yet to internalize the mechanisms underlying true mathematical discovery.
                </p>
            </div>
        </section>
        
        <!-- Benchmark info seciont -->
        <section class="chart-section" id="benchmark-Info">
            <h2 class="chart-title">Understanding the Benchmarks</h2>
            
            <div class="benchmark-info">
                <div class="info-card">
                    <h3>Why Multiple Benchmarks?</h3>
                    <p>No single benchmark can fully capture AI capabilities. Using multiple tests ensures a comprehensive evaluation of different cognitive abilities, from factual knowledge to reasoning and problem-solving.</p>
                </div>
                
                <div class="info-card">
                    <h3>Evaluation Methodology</h3>
                    <p>All models are tested under identical conditions with the same prompts and evaluation protocols. Results are verified through multiple runs to ensure statistical significance.</p>
                </div>
                
                <div class="info-card">
                    <h3>Limitations & Considerations</h3>
                    <p>Benchmarks have inherent limitations. They may not capture real-world performance, creativity, or ethical reasoning. Results should be interpreted alongside other evaluation methods.</p>
                </div>
            </div>

            <div class = "visualization-footer">
                <a href="details-bench.html" class="btn-primary">
                    More details
                    <span class="btn-arrow">→</span>
                </a>
            </div>
            
        </section>
    </div>
    
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <script src="../assets/js/polarChart.js"></script>
    <script src="../assets/js/barChart.js"></script>
</body>

</html>