<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Models Analysis</title>
    
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;700&display=swap" rel="stylesheet">
    
    <link rel="stylesheet" href="../assets/css/style.css">
    <link rel="stylesheet" href="../assets/css/pages.css">
</head>
<body class="subpage-body">

    <main class="container">
        <a href="../index.html#section-models" class="back-link">
            <span class="back-arrow">←</span> Back to Homepage
        </a>
        <header class="page-header">
            <h1>AI Models Analysis</h1>
            <p>Exploring the relationships between Size, Compute, and Training Time.</p>
        </header>

        <section class="chart-section">
            <h2>1. Model Size vs. Time</h2>
            <p>How the number of parameters has grown over time.</p>
            <div id="chart-params"></div>
            
            <div class="chart-analysis">
                <p>To comment on the graph, it's important to first understand how the scale works. While the x-axis represents the release dates of the various models, the y-axis represents the number of parameters of the individual models. The scale is logarithmic and ranges from 0 to more than 100 Trillions (k = thousands, M = millions, B = billions, T = Trillions). It's clear how for over fifty years the size of models grew slowly, then experienced some sudden surges in recent years, particularly at the beginning of the twenty-first century and over the last decade. Before 2000, the largest model ever created was Neocognitron, with 1.1 million parameters. Around that time, it began to be understood that investing in larger neural networks could lead to better results, and so before 2020, the largest model ever built had a size of 300 million parameters and was called SB-LM. In recent years, however, even large multinationals have begun investing in the AI ​​sector, contributing to the construction of very large models. It is estimated that the most recent models, such as Grok4, have several trillion parameters.</p>
            </div>
        </section>

        <section class="chart-section">
            <h2>2. Training Compute vs. Time</h2>
            <p>The total computational power (FLOPs) used to train the model.</p>
            <div id="chart-compute"></div>
            
            <div class="chart-analysis">
                <p>As in the previous graph, the y-axis scale is logarithmic in this case too: it goes from zero to more than 10^25 Floating Point Operations (FLOPs)! It's important to note that k stands for 10^3 FLOPs; we use B for 10^9 FLOPs, P for 10^15 FLOPs, E for 10^18 FLOPs, and Y for 10^24 FLOPs. The number of floating point operations appears to grow over time, following the same dynamics as in the previous graph. In this case, the trend appears even less "noisy" and follows an almost perfectly exponential trend over time. This is likely due to the fact that the FLOPs required to train models are closely tied to the hardware characteristics of supercomputers and are therefore limited. The number of parameters in a neural network, however, can be varied at will, although it has been noted over time that a model with more parameters does not necessarily obtain better results. To understand this, however, companies have experimented by building networks with very many or very few parameters, making the data from the first scatterplot "dirtier".</p>
            </div>
        </section>

        <section class="chart-section">
            <h2>3. Training Time & Dataset Size vs. Time</h2>
            <p>Position: Training Time (Hours). Bubble Size: Dataset Size (Gradients).</p>
            <div id="chart-bubble"></div>
            
            <div class="chart-analysis">
                <p>It's important to note that the data labeled "Dataset Size" is defined as "the smallest unit that provides a training/gradient update." While the overall trend is similar to the previous graphs, there are key differences. Training time has grown exponentially in recent years, though perhaps at a slower pace. In contrast, dataset size remained relatively stable for most of history, before exploding in the last five years. This is likely due to the rise of social networks and the emergence of companies specializing in data collection, which has recently become more heavily regulated.</p>
            </div>
        </section>

        <div id="tooltip" class="tooltip" style="opacity:0;"></div>
    </main>

    <footer class="page-footer">
        <div class="footer-content">
            <p>Source: <a href="https://epoch.ai/data/ai-models" target="_blank" rel="noopener">epoch.ai/data/ai-models</a></p>
        </div>
    </footer>

    <script src="https://d3js.org/d3.v7.min.js"></script>
    <script src="../assets/js/models.js"></script>

</body>
</html>